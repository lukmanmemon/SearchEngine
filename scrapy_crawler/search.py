import ast
import math
import numpy as np
import collections
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import pickle
import codecs
import requests
from urllib.parse import urlparse
import json

# Load the vars generated by the invert script for use while searching
with open('InvertVariableStore.pckl', 'rb') as loadable:
    stopwords_list, data_list, url_list, title_list, description_list = pickle.load(loadable)

with codecs.open('dict.txt', encoding='utf-8') as dict_file:
    dict_data = dict_file.read()

with codecs.open('postings.txt', encoding='utf-8') as postings_file:
    postings_data = postings_file.read()

# Convert from string to dictionary
dictionary = ast.literal_eval(dict_data)

# Convert postings list from string to dictionary
postings_list = ast.literal_eval(postings_data)


def search(query_input):
    # Split input query into tokens
    query_tokens = word_tokenize(query_input)

    # Filter query list and apply stemming/stopword removal
    query_terms = []
    ps = PorterStemmer()
    for word in query_tokens:
        word = ps.stem(word)
        
        if word in dictionary and word not in stopwords_list:
            query_terms.append(word)

    sorted_query_terms = sorted(query_terms)
    final_query_terms = []
    [final_query_terms.append(word) for word in sorted_query_terms if word not in final_query_terms]

    # Get query term frequencies
    query_terms_frequencies = {}
    for word in sorted_query_terms:
        if word in query_terms_frequencies:
            query_terms_frequencies[word] = query_terms_frequencies[word] + 1
        else:
            query_terms_frequencies[word] = 1

    # Add idf of query terms to dictionary
    terms_idf = {}
    num_of_pages = len(data_list)
    for word in final_query_terms:
        df = dictionary[word]
        idf = math.log(1 + (num_of_pages / df), 10)
        terms_idf[word] = idf

    # Get vectors for each document and store in dictionary
    document_vectors_dict = {}
    for id in range(num_of_pages):
        document_vector = np.zeros((len(final_query_terms)), dtype=float)
        index = 0
        for term in final_query_terms:
            # Get weight by idf * tf
            try:
                document_vector[index] = terms_idf[term] * (1 + math.log(postings_list[term][id][2], 10))
            except KeyError:
                document_vector[index] = 0
            index = index + 1

        document_vectors_dict[id] = document_vector

    # Calculate document vector lengths and store in dictionary
    document_vector_lengths = {}
    for id, vector in document_vectors_dict.items():
        sum = 0
        length = 0
        for weight in vector:
            sum = sum + weight ** 2
            
        length = math.sqrt(sum)
        document_vector_lengths[id] = length

    # Get query vector
    query_vector = np.zeros((len(final_query_terms)), dtype=float)
    q_index = 0
    for term in final_query_terms:
        # Get weight by idf * tf
        query_vector[q_index] = terms_idf[term] * (1 + math.log(query_terms_frequencies[term], 10))
        q_index = q_index + 1

    # Calculate query vector length
    qvector_sum = 0
    for weight in query_vector:
        qvector_sum = qvector_sum + weight ** 2

    qvector_length = math.sqrt(qvector_sum)
    
    # Store pageRankScores
    pagerank_scores = {}
    domains_to_pageRank = []
    
    for url in url_list:
        domain = urlparse(url).netloc
        if domain in pagerank_scores:
            pass
        else:
            domains_to_pageRank.append(domain)
            pagerank_scores[domain] = 0
    
    pagerank_query = "https://openpagerank.com/api/v1.0/getPageRank?"
    for domain in domains_to_pageRank:
        pagerank_query += "&domains[]=" + domain
    # print(pagerank_query) 
    
    # Fetch the PageRank scores
    headers = {
    'API-OPR': '0skw4gg444800ggc8ggw00wwso84cokkwg40wgs0 '
    }

    response = requests.request("GET", pagerank_query, headers=headers)
    
    responseJSON = json.loads(response.text)
    
    for res in responseJSON['response']:
        pagerank_scores[res['domain']] = res['page_rank_decimal']/10
    
    # print(pagerank_scores)

    # Calculate cosine similarity 
    final_similarity = {}
    final_score = {}
    for id in range(num_of_pages):
        # Similarity score is the cosine similarity
        similarity_score = 0 if document_vector_lengths[id] == 0 else (document_vectors_dict[id].dot(query_vector)) / (document_vector_lengths[id] * qvector_length)
        if math.isnan(similarity_score):
            similarity_score = 0
        
        pr_domain = urlparse(url_list[id]).netloc
        # Use similarity score in weighted formula to combine with PageRank. If they are both 0, then the overall relevance is 0
        if similarity_score == 0 and pagerank_scores[pr_domain] == 0:
            final_similarity[id] = 0
        else:
            # Using 70/30 weighting
            final_similarity[id] = (0.7 * similarity_score) + (0.3 * pagerank_scores[pr_domain])


    final_similarity_collection = collections.Counter(final_similarity)
    

    # Sort collection of scores in descending order
    # sorted_cosine_similarity = cosine_similarity_collection.most_common()
    


    # Get top K relevant documents
    topK_rel_documents = final_similarity_collection.most_common(10)
    
    rank = 1
    top_ret_documents = []
    for key, value in topK_rel_documents:
        top_ret_documents.append({"url": url_list[key], "title": title_list[key], "description": description_list[key]})
        rank = rank + 1
        

    return top_ret_documents


if __name__ == "__main__":
    print(search("Ryerson University"))
